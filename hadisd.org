#+PROPERTY: session *shell*
#+PROPERTY: results output
#+PROPERTY: exports both

* Download the files

THis had to be done manually in a browser at
http://www.metoffice.gov.uk/hadobs/hadisd/download_2012.html


* Unpack the data

#+BEGIN_SRC sh :results silent
  mkdir hadisd2012
#+END_SRC


#+BEGIN_SRC sh

  for file in WMO_???000-???999.tar.gz; do
    echo -n "$file : "
    tar --checkpoint=.100000 --skip-old-files --directory=hadisd2012 -xzf $file 2>&1
    echo
  done
  
#+END_SRC

#+RESULTS:
#+begin_example

> > > WMO_000000-099999.tar.gz : .....................
WMO_100000-199999.tar.gz : ...............
WMO_200000-299999.tar.gz : ..........
WMO_300000-399999.tar.gz : ...........
WMO_400000-499999.tar.gz : ...........
WMO_500000-599999.tar.gz : ........
WMO_600000-699999.tar.gz : ........
WMO_700000-719999.tar.gz : .................
WMO_720000-724999.tar.gz : ...........................
WMO_725000-729999.tar.gz : ....................
WMO_730000-799999.tar.gz : .......
WMO_800000-899999.tar.gz : ........
WMO_900000-999999.tar.gz : ........
#+end_example


* TODO Convert to NC4

Joshua did this. Get his commands.

* Some custom R code is necessary

CDO does not perform all of the operations we need.  The script below
accomplishes the following:

1. Converting fractional days that result from the unit conversion to
   integer days by applying a ceiling function.
2. Calculating the appropriate row/column file name according to the
   pSIMS convention using the global attributes.
3. Add latitude & longitude dimensional variables, using grid cell
   center point as values.

This script assumes that it is in the directory corresponding to a
station where the intermediate data resides.  It looks for the file
=merge.nc4=.

#+BEGIN_SRC R :session *R:2* :tangle hadisd.R :shebang #!/software/R-2.15-el6-x86_64/bin/Rscript
  library( raster)
  library( ncdf4)
  library( stringr)
  
  # setwd( "hadisd2012/hadisd.1.0.0.2011f.501360-99999")
  
  nc <- nc_open( "merge.nc4")
  
  lonLat <- matrix(
    c( x= ncatt_get( nc, 0, "longitude")$value,
      y= ncatt_get( nc, 0, "latitude")$value),
    nrow= 1)
  
  
  world <- raster()
  res( world) <- 30 / 60 / 60
  
  rowCol <- rowColFromCell(world, cellFromXY( world, lonLat))
  
  dailyPath <- with(
    as.list( rowCol[ 1,]),
    sprintf(
      "../../daily/%1$d/%2$d/%1$d_%2$d.psims.nc",
      row, col))
  
  dir.create(
    dirname( dailyPath),
    recursive= TRUE)
  
  station <- str_match(
    basename( getwd()),
    "\\.([0-9]{6})-[0-9]+$")[,2]
  
  if( station %in% c( "766340", "971800")) {
    dailyPath <- str_c( dailyPath, ".1")
  }
  
  if( station %in% c( "766342", "971820")) {
    dailyPath <- str_c( dailyPath, ".2")
  }
  
  ncdump <- pipe(
    str_c(
      "ncdump -fC -n ",
      str_replace( basename(dailyPath), "\\.nc$", ""),
      " merge.nc4"),
    open= "r")
  ncgen <- pipe( str_c( "ncgen -b -k1 -o ", dailyPath), open= "w")
  ## ncgen <- ""
  inTimeStanza <- FALSE
  inHeader <- TRUE
  cat(
    readLines( ncdump, n= 2),
    "\tlongitude = 1 ;",
    "\tlatitude = 1 ;",
    readLines( ncdump, n= 1),
    sep= "\n",
    file= ncgen)
  ## soFar <- 5
  
  ## options( warn= 2, error= recover)
  ## options( warn= 0, error= NULL)  # the defaults
  
  repeat {
    cdl <- readLines( ncdump, n= 1)
    if( length( cdl) == 0) {
      close( ncdump)
      close( ncgen)
      break
    }
    ## soFar <- soFar + 1
    if( inHeader) {
      if( str_detect( cdl, "^data:$")) {
        inHeader <- FALSE
        xy <- xyFromCell(
          world,
          cellFromRowCol( world, rowCol[1,1], rowCol[1,2]))
        cdl <- c(
          cdl,
          "",
          str_c(
            " longitude = ",
            as.character( xy[1,1]), " ;"),
          "",
          str_c(
            " latitude = ",
            as.character( xy[1,2]), " ;"))
      } else {
        if( str_detect( cdl, "^variables:$")) {
          cdl <- c(
            cdl,
            "\tdouble longitude(longitude) ;",
            "\t\tlongitude:units = \"degrees_east\" ;",
            "\t\tlongitude:long_name = \"longitude\" ;",
            "\tdouble latitude(latitude) ;",
            "\tlatitude:units = \"degrees_north\" ;",
            "\tlatitude:long_name = \"latitude\" ;")
        }
        if( ## suppressWarnings(
          str_detect( cdl[1], "^\\tdouble time\\(time\\) ;")) {
          cdl <- "\tinteger time(time) ;"
        } else {
          nonDimVar <- str_match(
            cdl, "^(\\t[^(]*\\(time)\\) ;")[ 1, 2]
          if( !is.na( nonDimVar)) {
            cdl <- str_c( nonDimVar, ", latitude, longitude) ;")
          }
        }
      }
    }
    if( !inTimeStanza) {
      ## inTimeStanza <- str_detect( cdl, perl( "^\\s*time ="))
      inTimeStanza <- str_detect( cdl[1], "^ *time =")
    }
    if( inTimeStanza) {
      if( str_detect( cdl[1], "^ *$")) {
        inTimeStanza <- FALSE
        ## break
      } else {
        cdlFields <- str_match(
          cdl,
          ## perl( "^(\\s*(time = )?)([.0-9]+)(.*)"))
          "^( *(time = )?)([.0-9]+)(.*)")
        cdl <- str_c(
          cdlFields[ 2],
          ceiling( as.numeric( cdlFields[ 4])),
          cdlFields[ 5])
      }
    }
    cat( cdl, sep= "\n", file= ncgen)
    ## if( soFar >= 200) break
  }
  
  ## closeAllConnections()
#+END_SRC


* Capture the steps for each station file in a script

This uses CDO to do the aggregation, cleaning up intermediate files
along the way.  The final step calls the script from the previous
section.

#+BEGIN_SRC sh :session *shell* :shebang #!/bin/bash :tangle hadisd.sh
  
  CDO='cdo -f nc4 -z zip'
  # FILE='hadisd.1.0.0.2011f.501360-99999.nc4'
  set -x
  FILE=$1
  
  pushd hadisd2012
  
  STATION=${FILE%.*}
  mkdir $STATION
  pushd $STATION
  
  CDO segfaults when combining setname with the others
  ${CDO} daymax -selname,temperatures ../${FILE} tmax.nc4.1
  ${CDO} setname,tmax tmax.nc4.1 tmax.nc4; rm tmax.nc4.1
  ${CDO} daymin -selname,temperatures ../${FILE} tmin.nc4.1 
  ${CDO} setname,tmin tmin.nc4.1 tmin.nc4; rm tmin.nc4.1
  ${CDO} daysum -selname,precip1_depth ../${FILE} precip.nc4.1
  ${CDO} setname,precip precip.nc4.1 precip.nc4; rm precip.nc4.1
  ${CDO} daymean -selname,windspeeds ../${FILE} wind.nc4.1
  ${CDO} setname,wind wind.nc4.1 wind.nc4; rm wind.nc4.1
  ${CDO} daymean -selname,dewpoints ../${FILE} dewp.nc4.1
  ${CDO} setname,dewp dewp.nc4.1 dewp.nc4; rm dewp.nc4.1
  ${CDO} merge {tmax,tmin,precip,wind,dewp}.nc4 merge.nc4.1
  ${CDO} setreftime,1973-01-01,00:00:00,days merge.nc4.1 merge.nc4; rm merge.nc4.1
  ../../hadisd.R
  
  popd
  popd
  
#+END_SRC


* In case I forget to load =parallel=

#+BEGIN_SRC emacs-lisp :results silent
  (setenv
   "PATH"
   (concat
    "/software/parallel-latest-all/bin"
    ":" (getenv "PATH")))
  (setenv
   "MANPATH"
   (concat
    "/software/parallel-latest-all/share/man"
    ":" (getenv "MANPATH")))
#+END_SRC

#+BEGIN_SRC sh :session *shell* :results silent
  module load parallel
#+END_SRC


* Write out the list of station data files

#+BEGIN_SRC sh :session :results silent
  cd hadisd2012
  # find . -name '*.nc4' > ../hadisd2012.files
  # don't want leading './'
  ls *.nc4 > ../hadisd2012.files
#+END_SRC

=parallel= offers ={.}= and ={/}= that may be useful if the speed of
=find= is required.  See =man parallel=.

#+BEGIN_SRC sh :session
  head hadisd2012.files
#+END_SRC

#+RESULTS:
#+begin_example
hadisd.1.0.0.2011f.501360-99999.nc4
hadisd.1.0.0.2011f.503530-99999.nc4
hadisd.1.0.0.2011f.504340-99999.nc4
hadisd.1.0.0.2011f.504680-99999.nc4
hadisd.1.0.0.2011f.505270-99999.nc4
hadisd.1.0.0.2011f.505480-99999.nc4
hadisd.1.0.0.2011f.505570-99999.nc4
hadisd.1.0.0.2011f.505640-99999.nc4
hadisd.1.0.0.2011f.506030-99999.nc4
hadisd.1.0.0.2011f.506320-99999.nc4
#+end_example


* Prepare a SLURM script that runs parallel

Taken from the [[http://docs.rcc.uchicago.edu/software/scheduler/parallel/README.html][RCC example]] of using GNU Parallel

#+BEGIN_SRC sh :tangle hadisd.sbatch :shebang #!/bin/sh
  #SBATCH --time=04:00:00
  #SBATCH --ntasks=160
  #SBATCH --exclusive
  #SBATCH --partition=sandyb,westmere,bigmem,amd
  module load parallel
#+END_SRC
  
The =--exclusive= to srun make =srun= use distinct CPUs for each job step.
=-N1 -n1= allocates a single core to each task.

#+BEGIN_SRC sh :tangle hadisd.sbatch
  # srun="srun --exclusive -N1 -n1"
  srun="srun --exclusive -N1 -n1 -c2"
#+END_SRC
  
=-j= is the number of tasks parallel runs so we set it to
=$SLURM_NTASKS=.  =--joblog= makes parallel create a log of tasks that
it has already run.  =--resume= makes parallel use the joblog to
resume from where it has left off.  The combination of =--joblog= and
=--resume= allow jobs to be resubmitted if necessary and continue from
where they left off.

#+BEGIN_SRC sh :tangle hadisd.sbatch
  parallel="parallel -j $SLURM_NTASKS --joblog log/parallel.log --resume"
  $parallel "$srun ./hadisd.sh {1} &> log/{.}.log" :::: hadisd2012.files
#+END_SRC

** TODO convert comments to prose

** TODO test whether =sbatch= can appear in the shebang.

* TODO Tangle and run

#+BEGIN_SRC emacs-lisp
;;tangle function here
#+END_SRC

#+BEGIN_SRC sh :session
  chmod ug+x hadisd.sh
  chmod ug+x hadisd.R
#+END_SRC

#+RESULTS:

#+BEGIN_SRC sh :session
  # rm runtask.log
  sbatch hadisd.sbatch
#+END_SRC

#+RESULTS:
: Submitted batch job 6948289

